{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a505db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, Dropout, BatchNormalization, ReLU\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "##WGAN-additional\n",
    "from keras import backend\n",
    "from keras.constraints import Constraint\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af572ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(datasettype):\n",
    "    if datasettype == 'mnist':\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "        x_train = (x_train[(y_train == 5) | (y_train == 7),]- 127.5) / 127.5\n",
    "        x_train = np.expand_dims(x_train, axis=3)\n",
    "        y_train = y_train[(y_train == 5) | (y_train == 7)]\n",
    "        y_train = np.where(y_train == 5, 0, 1).reshape((-1,1))\n",
    "\n",
    "        x_test = (x_test[(y_test == 5) | (y_test == 7),]- 127.5) / 127.5\n",
    "        x_test = np.expand_dims(x_test, axis=3)\n",
    "        y_test = y_test[(y_test == 5) | (y_test == 7)]\n",
    "        y_test = np.where(y_test == 5, 0, 1).reshape((-1,1))\n",
    "\n",
    "    elif datasettype == 'cifar10':\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "        y_train = np.squeeze(y_train)\n",
    "        y_test = np.squeeze(y_test)\n",
    "        x_train = (x_train[(y_train == 1) | (y_train == 7),]- 127.5) / 127.5\n",
    "        y_train = y_train[(y_train == 1) | (y_train == 7)]\n",
    "        y_train = np.where(y_train == 1, 0, 1).reshape((-1,1))\n",
    "\n",
    "        x_test = (x_test[(y_test == 1) | (y_test == 7),]- 127.5) / 127.5\n",
    "        y_test = y_test[(y_test == 1) | (y_test == 7)]\n",
    "        y_test = np.where(y_test == 1, 0, 1).reshape((-1,1))\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def makediscriminator(input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (5,5), strides=(2, 2), padding='same', input_shape = input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (5,5), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    ### WGAN-Change to linear activation\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    ###WGAN-Use RMSPROP\n",
    "    # opt = RMSprop(lr=0.00005)\n",
    "    ### WGAN-Wasserstein Loss\n",
    "    # model.compile(loss=wasserstein_loss, optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def makegenerator(datasettype, latent_dim):\n",
    "    if datasettype == 'mnist':\n",
    "        channels = 1\n",
    "        finalsize = 28\n",
    "    elif datasettype == 'cifar10':\n",
    "        channels = 3\n",
    "        finalsize = 32\n",
    "\n",
    "    model = Sequential()\n",
    "    if datasettype == 'mnist':\n",
    "\n",
    "        model.add(Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Reshape((7, 7, 256)))\n",
    "\n",
    "        model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "\n",
    "    elif datasettype == 'cifar10':\n",
    "        n_nodes = 256 * 4 * 4\n",
    "        model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Reshape((4, 4, 256)))\n",
    "        # upsample to 8x8\n",
    "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        # upsample to 16x16\n",
    "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        # upsample to 32x32\n",
    "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        # output layer\n",
    "        model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "def gen_real(dataset, n_samples):\n",
    "    # generate random indices for subsampling\n",
    "    idx = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    x = dataset[idx]\n",
    "    ### for WGAN generate class labels == -1 for real\n",
    "    y = -np.ones((n_samples, 1))\n",
    "    return x, y\n",
    "\n",
    "def gen_fake(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = get_latent(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    ### for WGAN generate class labels == 1 for fake\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "def get_latent(latent_dim, n_samples):\n",
    "    # generate latent input points\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    # reshape to fit network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "def save_plot(savepath, examples, epoch, n=4):\n",
    "    # plot images\n",
    "    print(examples.shape)\n",
    "    print(np.min(examples))\n",
    "    print(np.max(examples))\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        plt.axis('off')\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(examples[i].astype('uint8'))\n",
    "    # save plot to file\n",
    "    filename = 'Generated_plot_e%03d.png' % (epoch+1)\n",
    "    plt.savefig(savepath + filename)\n",
    "    plt.close()\n",
    "\n",
    "def performance(savepath, epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "    # prepare real samples\n",
    "    # X_real, y_real = gen_real(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    # _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, _ = gen_fake(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    # _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    # print('Performance test> Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save plot\n",
    "    fakeplot = (x_fake * 127.5) + 127.5\n",
    "\n",
    "    save_plot(savepath, fakeplot, epoch)\n",
    "    # save the generator model tile file\n",
    "    g_filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "    g_model.save(savepath + g_filename)\n",
    "    d_filename = 'discriminator_model_%03d.h5' % (epoch + 1)\n",
    "    d_model.save(savepath + d_filename)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    gen_loss = tf.math.reduce_mean(fake_output)\n",
    "    return gen_loss\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    loss = tf.math.reduce_mean(real_output) - tf.math.reduce_mean(fake_output)\n",
    "    return loss\n",
    "\n",
    "def train(datasettype, savepath, x_train, latent_dim, n_critic=5, n_epochs=200, batchsize=256, retries = 5, max_loss_increase_epochs = 10):\n",
    "    batch_per_epoch = int(x_train.shape[0] / batchsize)\n",
    "     # manually enumerate epochs\n",
    "    \n",
    "    for r in range(retries):\n",
    "        print(f'Attempt:{r+1}')\n",
    "        d_model = makediscriminator(x_train.shape[1:])\n",
    "        g_model = makegenerator(datasettype, latent_dim)\n",
    "\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        g_loss_epoch = []\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            print(f'Epoch: {i+1}')\n",
    "            # enumerate batches over the training set\n",
    "            for j in tqdm(range(batch_per_epoch)):\n",
    "                \n",
    "                ### WGAN-update critic more times than generator\n",
    "                for _ in range(n_critic):\n",
    "                    # get randomly selected 'real' samples\n",
    "                    X_real, y_real = gen_real(x_train, batchsize)\n",
    "                    # generate 'fake' examples\n",
    "                    X_fake, y_fake = gen_fake(g_model, latent_dim, batchsize)\n",
    "                    # update discriminator model weights\n",
    "\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        # tape.watch(d_model.trainable_variables)\n",
    "                        d_real_output = d_model(X_real, training = True)\n",
    "                        d_fake_output = d_model(X_fake, training = True)\n",
    "\n",
    "                        d_loss = discriminator_loss(d_real_output, d_fake_output)\n",
    "\n",
    "                    gradients_d_model = tape.gradient(d_loss, d_model.trainable_variables)\n",
    "                    \n",
    "                    opt = RMSprop(learning_rate=0.00005)\n",
    "\n",
    "                    opt.apply_gradients(zip(gradients_d_model, d_model.trainable_variables))\n",
    "                \n",
    "                # clip weights\n",
    "                for w in d_model.trainable_variables:\n",
    "                    w.assign(tf.clip_by_value(w, -0.01, 0.01))\n",
    "                  \n",
    "                # prepare points in latent space as input for the generator\n",
    "                X_gan = get_latent(latent_dim, batchsize)\n",
    "                ###WGAN- minust labels\n",
    "                y_gan = -np.ones((batchsize, 1))\n",
    "\n",
    "                ### WGAN-update generator\n",
    "                with tf.GradientTape() as gen_tape:\n",
    "                    # tape.watch(g_model.trainable_variables)\n",
    "                    generated_images = g_model(X_gan, training=True)\n",
    "                    g_fake_output = d_model(generated_images, training=False)\n",
    "                    g_loss = generator_loss(g_fake_output)\n",
    "                \n",
    "                gradients_g_model = gen_tape.gradient(g_loss, g_model.trainable_variables)\n",
    "                opt = RMSprop(learning_rate=0.00005)\n",
    "\n",
    "                opt.apply_gradients(zip(gradients_g_model, g_model.trainable_variables))\n",
    "\n",
    "                # summarize loss on this batch\n",
    "                g_losses.append(g_loss)\n",
    "                d_losses.append(d_loss)\n",
    "\n",
    "            print('Try:%d, Epoch:%d, D_Loss=%.3f, G_Loss=%.3f\\n' % (r+1, i+1, d_loss, g_loss))\n",
    "            g_loss_epoch.append(g_loss)\n",
    "            # if len(g_loss_epoch) >= 2:\n",
    "            #     if g_loss_epoch[-1] >  g_loss_epoch[-2]:\n",
    "            #         g_loss_inc_counter += 1\n",
    "            #         print(f'Generator loss increased for {g_loss_inc_counter} epochs.')\n",
    "            #     elif g_loss_epoch[-1] <  g_loss_epoch[-2]:\n",
    "            #         g_loss_inc_counter = 0\n",
    "\n",
    "            # if g_loss_inc_counter == max_loss_increase_epochs:\n",
    "            #     print(f'Try: {r+1} failed to train. Restarting training')\n",
    "            #     break\n",
    "    #         evaluate the model performance, sometimes\n",
    "            if (i+1) % 20 == 0:\n",
    "                performance(savepath, i, g_model, d_model, x_train, latent_dim)\n",
    "\n",
    "            if i == n_epochs - 1:\n",
    "                return g_model, d_model, d_losses, g_losses\n",
    "\n",
    "    # reach here if fail to train\n",
    "    print(\"Training Stopped...\")\n",
    "    return g_model, d_model, d_losses, g_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e57739",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = loaddata('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4edf450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = './testwgan/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "053d80ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt:1\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:33<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:1, D_Loss=-39.833, G_Loss=-18.450\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:34<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:2, D_Loss=-28.498, G_Loss=-0.081\n",
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:36<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:3, D_Loss=-0.434, G_Loss=-4.158\n",
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:36<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:4, D_Loss=-0.096, G_Loss=-1.137\n",
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:36<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:5, D_Loss=-0.073, G_Loss=-1.006\n",
      "\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:36<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:6, D_Loss=-0.073, G_Loss=-0.825\n",
      "\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:36<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:7, D_Loss=-0.092, G_Loss=-1.079\n",
      "\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:36<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:8, D_Loss=-0.077, G_Loss=-0.996\n",
      "\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:37<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:9, D_Loss=-0.019, G_Loss=-0.745\n",
      "\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:35<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:10, D_Loss=-0.054, G_Loss=-0.044\n",
      "\n",
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:32<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:11, D_Loss=-0.091, G_Loss=-1.331\n",
      "\n",
      "Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:32<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:12, D_Loss=-0.054, G_Loss=-0.185\n",
      "\n",
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:32<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:13, D_Loss=-0.091, G_Loss=-1.638\n",
      "\n",
      "Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:32<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:14, D_Loss=0.007, G_Loss=-0.644\n",
      "\n",
      "Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:32<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:15, D_Loss=-0.035, G_Loss=0.026\n",
      "\n",
      "Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:34<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:16, D_Loss=-0.106, G_Loss=0.213\n",
      "\n",
      "Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:33<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:17, D_Loss=-0.076, G_Loss=0.110\n",
      "\n",
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:33<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:18, D_Loss=-0.120, G_Loss=0.094\n",
      "\n",
      "Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:33<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:19, D_Loss=-0.039, G_Loss=-0.229\n",
      "\n",
      "Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:35<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try:1, Epoch:20, D_Loss=-0.024, G_Loss=-0.044\n",
      "\n",
      "(100, 28, 28, 1)\n",
      "0.0\n",
      "255.0\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 107/182 [00:21<00:14,  5.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Assignments\\DSA5204\\Project\\Develop - WGAN Tape.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000004?line=0'>1</a>\u001b[0m g_model, d_model, d_loss, g_loss \u001b[39m=\u001b[39m \\\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000004?line=1'>2</a>\u001b[0m train(\u001b[39m'\u001b[39;49m\u001b[39mmnist\u001b[39;49m\u001b[39m'\u001b[39;49m, savepath, x_train, latent_dim\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, n_critic\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, batchsize\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, retries \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, max_loss_increase_epochs \u001b[39m=\u001b[39;49m \u001b[39m10000\u001b[39;49m)\n",
      "\u001b[1;32md:\\Assignments\\DSA5204\\Project\\Develop - WGAN Tape.ipynb Cell 2'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(datasettype, savepath, x_train, latent_dim, n_critic, n_epochs, batchsize, retries, max_loss_increase_epochs)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=205'>206</a>\u001b[0m X_real, y_real \u001b[39m=\u001b[39m gen_real(x_train, batchsize)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=206'>207</a>\u001b[0m \u001b[39m# generate 'fake' examples\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=207'>208</a>\u001b[0m X_fake, y_fake \u001b[39m=\u001b[39m gen_fake(g_model, latent_dim, batchsize)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=208'>209</a>\u001b[0m \u001b[39m# update discriminator model weights\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=210'>211</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=211'>212</a>\u001b[0m     \u001b[39m# tape.watch(d_model.trainable_variables)\u001b[39;00m\n",
      "\u001b[1;32md:\\Assignments\\DSA5204\\Project\\Develop - WGAN Tape.ipynb Cell 2'\u001b[0m in \u001b[0;36mgen_fake\u001b[1;34m(g_model, latent_dim, n_samples)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=122'>123</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgen_fake\u001b[39m(g_model, latent_dim, n_samples):\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=123'>124</a>\u001b[0m     \u001b[39m# generate points in latent space\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=124'>125</a>\u001b[0m     x_input \u001b[39m=\u001b[39m get_latent(latent_dim, n_samples)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=125'>126</a>\u001b[0m     \u001b[39m# predict outputs\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=126'>127</a>\u001b[0m     X \u001b[39m=\u001b[39m g_model\u001b[39m.\u001b[39mpredict(x_input)\n",
      "\u001b[1;32md:\\Assignments\\DSA5204\\Project\\Develop - WGAN Tape.ipynb Cell 2'\u001b[0m in \u001b[0;36mget_latent\u001b[1;34m(latent_dim, n_samples)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=131'>132</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_latent\u001b[39m(latent_dim, n_samples):\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=132'>133</a>\u001b[0m     \u001b[39m# generate latent input points\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=133'>134</a>\u001b[0m     x_input \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandn(latent_dim \u001b[39m*\u001b[39;49m n_samples)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=134'>135</a>\u001b[0m     \u001b[39m# reshape to fit network\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Assignments/DSA5204/Project/Develop%20-%20WGAN%20Tape.ipynb#ch0000001?line=135'>136</a>\u001b[0m     x_input \u001b[39m=\u001b[39m x_input\u001b[39m.\u001b[39mreshape(n_samples, latent_dim)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g_model, d_model, d_loss, g_loss = \\\n",
    "train('mnist', savepath, x_train, latent_dim=100, n_critic=5, n_epochs=200, batchsize=256, retries = 5, max_loss_increase_epochs = 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
